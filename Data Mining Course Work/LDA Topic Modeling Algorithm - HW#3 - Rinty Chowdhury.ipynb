{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC495 - DataMining - HW#3 - Rinty Chowdhury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "import gensim.corpora as corpora\n",
    "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_numeric, remove_stopwords, strip_multiple_whitespaces, strip_short\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.field_size_limit(1000000000)\n",
    "my_data = pd.read_csv('./state-of-the-union.csv', names=['Year', 'Text'])\n",
    "my_data = my_data.replace('\\n',' ', regex=True)\n",
    "my_data = my_data.groupby(['Year'])['Text'].apply(', '.join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "century_20th = my_data[(my_data.Year >= 1901) & (my_data.Year <= 2000)].reset_index()\n",
    "del century_20th['index']\n",
    "\n",
    "century_21st = my_data[(my_data.Year >= 2001) & (my_data.Year <= 2012)].reset_index()\n",
    "del century_21st['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "decade1_20 = century_20th[(century_20th.Year >= 1901) & (century_20th.Year <= 1910)]\n",
    "del decade1_20['Year']\n",
    "decade1_20 = decade1_20['Text'].str.cat(sep=', ')\n",
    "\n",
    "decade2_20 = century_20th[(century_20th.Year >= 1911) & (century_20th.Year <= 1920)]\n",
    "del decade2_20['Year']\n",
    "decade2_20 = decade2_20['Text'].str.cat(sep=', ')\n",
    "\n",
    "decade3_20 = century_20th[(century_20th.Year >= 1921) & (century_20th.Year <= 1930)]\n",
    "del decade3_20['Year']\n",
    "decade3_20 = decade3_20['Text'].str.cat(sep=', ')\n",
    "\n",
    "decade4_20 = century_20th[(century_20th.Year >= 1931) & (century_20th.Year <= 1940)]\n",
    "del decade4_20['Year']\n",
    "decade4_20 = decade4_20['Text'].str.cat(sep=', ')\n",
    "\n",
    "decade5_20 = century_20th[(century_20th.Year >= 1941) & (century_20th.Year <= 1950)]\n",
    "del decade5_20['Year']\n",
    "decade5_20 = decade5_20['Text'].str.cat(sep=', ')\n",
    "\n",
    "decade6_20 = century_20th[(century_20th.Year >= 1951) & (century_20th.Year <= 1960)]\n",
    "del decade6_20['Year']\n",
    "decade6_20 = decade6_20['Text'].str.cat(sep=', ')\n",
    "\n",
    "decade7_20 = century_20th[(century_20th.Year >= 1961) & (century_20th.Year <= 1970)]\n",
    "del decade7_20['Year']\n",
    "decade7_20 = decade7_20['Text'].str.cat(sep=', ')\n",
    "\n",
    "decade8_20 = century_20th[(century_20th.Year >= 1971) & (century_20th.Year <= 1980)]\n",
    "del decade8_20['Year']\n",
    "decade8_20 = decade8_20['Text'].str.cat(sep=', ')\n",
    "\n",
    "decade9_20 = century_20th[(century_20th.Year >= 1981) & (century_20th.Year <= 1990)]\n",
    "del decade9_20['Year']\n",
    "decade9_20 = decade9_20['Text'].str.cat(sep=', ')\n",
    "\n",
    "decade10_20 = century_20th[(century_20th.Year >= 1991) & (century_20th.Year <= 2000)]\n",
    "del decade10_20['Year']\n",
    "decade10_20 = decade10_20['Text'].str.cat(sep=', ')\n",
    "\n",
    "d1 = {'Decade': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'Text': [decade1_20, decade2_20, \n",
    "                decade3_20, decade4_20, decade5_20, decade6_20, decade7_20, decade8_20, decade9_20, decade10_20]}\n",
    "decade_20th_century = pd.DataFrame(data=d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "decade1_21 = century_21st[(century_21st.Year >= 2001) & (century_21st.Year <= 2010)]\n",
    "del decade1_21['Year']\n",
    "decade1_21 = decade1_21['Text'].str.cat(sep=', ')\n",
    "\n",
    "decade2_21 = century_21st[(century_21st.Year >= 2011) & (century_21st.Year <= 2012)]\n",
    "del decade2_21['Year']\n",
    "decade2_21 = decade2_21['Text'].str.cat(sep=', ')\n",
    "\n",
    "d2 = {'Decade': [1, 2], 'Text': [decade1_21, decade2_21]}\n",
    "decade_21st_century = pd.DataFrame(data=d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range (0, len(my_data['Text'])):\n",
    "    my_data.Text[i] = str(my_data.Text[i])\n",
    "\n",
    "for i in range (0, len(decade_20th_century['Text'])):\n",
    "    decade_20th_century.Text[i] = str(decade_20th_century.Text[i])\n",
    "\n",
    "for i in range (0, len(decade_21st_century['Text'])):\n",
    "    decade_21st_century.Text[i] = str(decade_21st_century.Text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_numeric, remove_stopwords, \n",
    "                  strip_multiple_whitespaces, strip_short]\n",
    "\n",
    "for i in range (0, len(my_data['Text'])):\n",
    "    my_data.Text[i] = preprocess_string(my_data.Text[i], CUSTOM_FILTERS)\n",
    "    \n",
    "for i in range (0, len(decade_20th_century['Text'])):\n",
    "    decade_20th_century.Text[i] = preprocess_string(decade_20th_century.Text[i], CUSTOM_FILTERS)\n",
    "    \n",
    "for i in range (0, len(decade_21st_century['Text'])):\n",
    "    decade_21st_century.Text[i] = preprocess_string(decade_21st_century.Text[i], CUSTOM_FILTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for m in range (0, len(my_data['Text'])):\n",
    "    for n in range (0, len(my_data.Text[m])):\n",
    "        my_data.Text[m][n] = lemmatizer.lemmatize(my_data.Text[m][n])\n",
    "        \n",
    "for m in range (0, len(decade_20th_century['Text'])):\n",
    "    for n in range (0, len(decade_20th_century.Text[m])):\n",
    "        decade_20th_century.Text[m][n] = lemmatizer.lemmatize(decade_20th_century.Text[m][n])\n",
    "        \n",
    "for m in range (0, len(decade_21st_century['Text'])):\n",
    "    for n in range (0, len(decade_21st_century.Text[m])):\n",
    "        decade_21st_century.Text[m][n] = lemmatizer.lemmatize(decade_21st_century.Text[m][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = my_data['Text']\n",
    "data_20th = decade_20th_century['Text']\n",
    "data_21st = decade_21st_century['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = Dictionary(data)\n",
    "id2word.filter_extremes(no_below=20, no_above=0.5)\n",
    "corpus = [id2word.doc2bow(text) for text in data]\n",
    "\n",
    "id2word_20th = Dictionary(data_20th)\n",
    "id2word_20th.filter_extremes(no_below=4, no_above=0.5)\n",
    "corpus_20th = [id2word_20th.doc2bow(text) for text in data_20th]\n",
    "\n",
    "id2word_21st = Dictionary(data_21st)\n",
    "corpus_21st = [id2word_21st.doc2bow(text) for text in data_21st]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=20, iterations = 400, chunksize=2000,\n",
    "                    passes=10, alpha='auto', eta='auto', per_word_topics=True, eval_every = None)\n",
    "\n",
    "lda_model_20th = LdaModel(corpus=corpus_20th, id2word=id2word_20th, num_topics=10, iterations = 400, chunksize=2000,\n",
    "                    passes=10, alpha='auto', eta='auto', per_word_topics=True, eval_every = None)\n",
    "\n",
    "lda_model_21st = LdaModel(corpus=corpus_21st, id2word=id2word_21st, num_topics=2, iterations = 400, chunksize=2000,\n",
    "                    passes=10, alpha='auto', eta='auto', per_word_topics=True, eval_every = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.006*\"program\" + 0.005*\"resolution\" + 0.005*\"requires\" + 0.005*\"today\" + '\n",
      "  '0.005*\"productive\" + 0.005*\"area\" + 0.004*\"attitude\" + 0.004*\"humanity\" + '\n",
      "  '0.004*\"neighbor\" + 0.004*\"capability\"'),\n",
      " (1,\n",
      "  '0.025*\"program\" + 0.011*\"billion\" + 0.007*\"budget\" + 0.007*\"farm\" + '\n",
      "  '0.006*\"area\" + 0.006*\"level\" + 0.005*\"wage\" + 0.005*\"today\" + 0.005*\"major\" '\n",
      "  '+ 0.005*\"help\"'),\n",
      " (2,\n",
      "  '0.014*\"help\" + 0.013*\"job\" + 0.012*\"tonight\" + 0.009*\"school\" + '\n",
      "  '0.009*\"budget\" + 0.007*\"cut\" + 0.007*\"program\" + 0.006*\"today\" + '\n",
      "  '0.006*\"challenge\" + 0.005*\"deficit\"'),\n",
      " (3,\n",
      "  '0.015*\"enemy\" + 0.013*\"fighting\" + 0.009*\"attack\" + 0.009*\"victory\" + '\n",
      "  '0.008*\"japanese\" + 0.006*\"air\" + 0.006*\"job\" + 0.006*\"german\" + '\n",
      "  '0.006*\"democracy\" + 0.006*\"battle\"'),\n",
      " (4,\n",
      "  '0.009*\"gold\" + 0.007*\"note\" + 0.007*\"currency\" + 0.005*\"silver\" + '\n",
      "  '0.004*\"circulation\" + 0.004*\"bond\" + 0.003*\"tariff\" + 0.003*\"mexico\" + '\n",
      "  '0.003*\"paper\" + 0.003*\"article\"'),\n",
      " (5,\n",
      "  '0.004*\"intercourse\" + 0.004*\"tribe\" + 0.003*\"article\" + 0.003*\"convention\" '\n",
      "  '+ 0.003*\"execution\" + 0.003*\"spain\" + 0.003*\"navigation\" + '\n",
      "  '0.003*\"disposition\" + 0.002*\"french\" + 0.002*\"commissioner\"'),\n",
      " (6,\n",
      "  '0.014*\"management\" + 0.011*\"dispute\" + 0.010*\"strike\" + 0.010*\"housing\" + '\n",
      "  '0.009*\"collective\" + 0.008*\"veteran\" + 0.008*\"commissioner\" + '\n",
      "  '0.006*\"program\" + 0.005*\"gentleman\" + 0.005*\"boundary\"'),\n",
      " (7,\n",
      "  '0.016*\"cent\" + 0.009*\"wage\" + 0.008*\"estimated\" + 0.008*\"ton\" + '\n",
      "  '0.006*\"depression\" + 0.006*\"export\" + 0.005*\"tariff\" + 0.005*\"contract\" + '\n",
      "  '0.005*\"commissioner\" + 0.005*\"import\"'),\n",
      " (8,\n",
      "  '0.020*\"mexico\" + 0.008*\"texas\" + 0.006*\"mexican\" + 0.005*\"convention\" + '\n",
      "  '0.005*\"article\" + 0.004*\"spain\" + 0.004*\"california\" + 0.004*\"enemy\" + '\n",
      "  '0.003*\"kansa\" + 0.003*\"whilst\"'),\n",
      " (9,\n",
      "  '0.006*\"belligerent\" + 0.006*\"convention\" + 0.006*\"function\" + '\n",
      "  '0.006*\"harbor\" + 0.006*\"jefferson\" + 0.006*\"upward\" + 0.006*\"ohio\" + '\n",
      "  '0.006*\"louisiana\" + 0.005*\"injury\" + 0.005*\"intercourse\"'),\n",
      " (10,\n",
      "  '0.008*\"democracy\" + 0.007*\"method\" + 0.006*\"industrial\" + 0.006*\"today\" + '\n",
      "  '0.006*\"budget\" + 0.006*\"group\" + 0.005*\"help\" + 0.005*\"democratic\" + '\n",
      "  '0.005*\"wage\" + 0.004*\"fear\"'),\n",
      " (11,\n",
      "  '0.009*\"forest\" + 0.008*\"corporation\" + 0.007*\"island\" + 0.006*\"industrial\" '\n",
      "  '+ 0.005*\"philippine\" + 0.005*\"wage\" + 0.005*\"merely\" + 0.005*\"worker\" + '\n",
      "  '0.004*\"interstate\" + 0.004*\"wealth\"'),\n",
      " (12,\n",
      "  '0.026*\"program\" + 0.010*\"soviet\" + 0.010*\"area\" + 0.008*\"assistance\" + '\n",
      "  '0.008*\"major\" + 0.007*\"help\" + 0.006*\"nuclear\" + 0.006*\"oil\" + '\n",
      "  '0.006*\"budget\" + 0.005*\"woman\"'),\n",
      " (13,\n",
      "  '0.008*\"island\" + 0.004*\"currency\" + 0.004*\"philippine\" + 0.004*\"rebellion\" '\n",
      "  '+ 0.003*\"paper\" + 0.003*\"cent\" + 0.003*\"seven\" + 0.003*\"slavery\" + '\n",
      "  '0.003*\"note\" + 0.003*\"white\"'),\n",
      " (14,\n",
      "  '0.006*\"island\" + 0.005*\"spain\" + 0.005*\"cuba\" + 0.005*\"convention\" + '\n",
      "  '0.004*\"commissioner\" + 0.004*\"pension\" + 0.004*\"gold\" + 0.004*\"silver\" + '\n",
      "  '0.003*\"bond\" + 0.003*\"cent\"'),\n",
      " (15,\n",
      "  '0.015*\"program\" + 0.012*\"goal\" + 0.011*\"today\" + 0.009*\"budget\" + '\n",
      "  '0.008*\"billion\" + 0.007*\"tonight\" + 0.007*\"achieve\" + 0.007*\"job\" + '\n",
      "  '0.006*\"percent\" + 0.006*\"level\"'),\n",
      " (16,\n",
      "  '0.007*\"corporation\" + 0.006*\"judge\" + 0.006*\"interstate\" + '\n",
      "  '0.006*\"conference\" + 0.005*\"railroad\" + 0.005*\"industrial\" + '\n",
      "  '0.004*\"combination\" + 0.004*\"farmer\" + 0.004*\"island\" + 0.004*\"merely\"'),\n",
      " (17,\n",
      "  '0.006*\"method\" + 0.006*\"tariff\" + 0.005*\"cent\" + 0.004*\"canal\" + '\n",
      "  '0.004*\"board\" + 0.004*\"conference\" + 0.004*\"statute\" + 0.004*\"bureau\" + '\n",
      "  '0.003*\"company\" + 0.003*\"panama\"'),\n",
      " (18,\n",
      "  '0.009*\"soviet\" + 0.008*\"railway\" + 0.007*\"communist\" + 0.007*\"industrial\" + '\n",
      "  '0.005*\"korea\" + 0.005*\"farmer\" + 0.005*\"recovery\" + 0.004*\"ruler\" + '\n",
      "  '0.004*\"study\" + 0.004*\"method\"'),\n",
      " (19,\n",
      "  '0.009*\"task\" + 0.006*\"domain\" + 0.006*\"profitable\" + 0.006*\"speak\" + '\n",
      "  '0.005*\"convention\" + 0.005*\"build\" + 0.005*\"possession\" + 0.005*\"fear\" + '\n",
      "  '0.004*\"constructive\" + 0.004*\"spent\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the top 20 topics:\n",
    "0.  It’s talking about a vision that requires all American to help their neighbors and be productive in their area.\n",
    "1.\tThis topic is related to allocating budget in the farm sector which will increase the wages of the workers.\n",
    "2.\tthis topic is related to a cut in the budget for school related fund which poses a challenge to skillful workforce.\n",
    "3.\tIt’s talking about Pearl Harbor attack when the Japanese attacked US battleship. They joined the German Nazi force to fight against US in WWII.\n",
    "4.\tThis topic is related to the federal reserve of currency, coins (gold & silver) and bond. It’s also talking about tariffs to be imposed for trade with Mexico.\n",
    "5.\tSpanish Army invaded Mexican region and colonized mating with the tribes.\n",
    "6.\tDue to mismanagement allegation in housing, veterans collectively went to a strike.\n",
    "7.\tWhen export vs import imbalance increased, the country went to a depression. Wages also remained stagnant.\n",
    "8.\tMexico was at war with Spain for independence. Texas and California became annexed to US.\n",
    "9.\tNo logical conclusion can be drawn\n",
    "10. With the rapid industrial growth, people with less machine skills are fearful of losing their jobs.\n",
    "11. In this Capitalist economic boom, the industrial big corporations became wealthy while average worker’s wage merely increased.\n",
    "12. During the nuclear arms race with Soviet Union, government assistance was provided to oil companies to provide fuel for US forces.\n",
    "13. No logical conclusion can be drawn\n",
    "14.\tThis is discussing three separate topics. One of them is relationship with Cuba. The other one is political convention. The last one is federal reserve of currency, coin, and bond.\n",
    "15.\tTonight, the President is describing how billions allocated to the budget will achieve unprecedented level of job growth.\n",
    "16.\tSimilar to topic 11.\n",
    "17.\tFederal bureau of investigation was investigating some cases of tax evasion by US companies in Panama.\n",
    "18.\tBoth Soviet and Korean communist rulers made significant improvement of nation-wide railway and industrial complex. But farmers suffered under their regime.\n",
    "19.\tNo logical conclusion can be drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.001*\"atomic\" + 0.001*\"missile\" + 0.001*\"wartime\" + 0.001*\"ruler\" + '\n",
      "  '0.001*\"manpower\" + 0.001*\"answered\" + 0.001*\"satellite\" + 0.001*\"nuclear\" + '\n",
      "  '0.001*\"aggressor\" + 0.001*\"ballistic\"'),\n",
      " (1,\n",
      "  '0.010*\"speculation\" + 0.008*\"ruler\" + 0.008*\"livelihood\" + '\n",
      "  '0.008*\"definitely\" + 0.008*\"franklin\" + 0.008*\"compelled\" + '\n",
      "  '0.006*\"intelligent\" + 0.006*\"neutrality\" + 0.006*\"panic\" + '\n",
      "  '0.006*\"sentiment\"'),\n",
      " (2,\n",
      "  '0.046*\"tonight\" + 0.028*\"nuclear\" + 0.012*\"vietnam\" + 0.011*\"missile\" + '\n",
      "  '0.009*\"sector\" + 0.008*\"medicare\" + 0.007*\"funding\" + 0.007*\"nato\" + '\n",
      "  '0.007*\"capability\" + 0.005*\"classroom\"'),\n",
      " (3,\n",
      "  '0.015*\"arbitration\" + 0.009*\"cruiser\" + 0.006*\"tribunal\" + 0.006*\"manifest\" '\n",
      "  '+ 0.006*\"objection\" + 0.005*\"rico\" + 0.005*\"commissioner\" + '\n",
      "  '0.005*\"postmaster\" + 0.005*\"workman\" + 0.005*\"ordinary\"'),\n",
      " (4,\n",
      "  '0.001*\"supervision\" + 0.001*\"enlisted\" + 0.001*\"colombia\" + '\n",
      "  '0.001*\"arbitration\" + 0.001*\"practicable\" + 0.001*\"ordinary\" + '\n",
      "  '0.001*\"disturbance\" + 0.001*\"tribunal\" + 0.001*\"undesirable\" + '\n",
      "  '0.001*\"exercised\"'),\n",
      " (5,\n",
      "  '0.010*\"freight\" + 0.008*\"gratifying\" + 0.006*\"league\" + 0.006*\"ruler\" + '\n",
      "  '0.006*\"tribunal\" + 0.006*\"negro\" + 0.006*\"cotton\" + 0.005*\"acreage\" + '\n",
      "  '0.005*\"exceedingly\" + 0.005*\"muscle\"'),\n",
      " (6,\n",
      "  '0.023*\"atomic\" + 0.016*\"wartime\" + 0.010*\"manpower\" + 0.010*\"liquidation\" + '\n",
      "  '0.009*\"businessmen\" + 0.007*\"postwar\" + 0.007*\"offensive\" + 0.006*\"kingdom\" '\n",
      "  '+ 0.006*\"moscow\" + 0.006*\"inflationary\"'),\n",
      " (7,\n",
      "  '0.012*\"supervision\" + 0.007*\"exercised\" + 0.006*\"naturalization\" + '\n",
      "  '0.006*\"enlisted\" + 0.005*\"colombia\" + 0.005*\"ordinary\" + 0.005*\"monroe\" + '\n",
      "  '0.005*\"timber\" + 0.005*\"arbitration\" + 0.005*\"tribunal\"'),\n",
      " (8,\n",
      "  '0.002*\"tonight\" + 0.001*\"nuclear\" + 0.001*\"medicare\" + 0.001*\"sector\" + '\n",
      "  '0.001*\"funding\" + 0.001*\"classroom\" + 0.001*\"tough\" + 0.001*\"yes\" + '\n",
      "  '0.001*\"vietnam\" + 0.001*\"capability\"'),\n",
      " (9,\n",
      "  '0.002*\"tonight\" + 0.001*\"nuclear\" + 0.001*\"arbitration\" + '\n",
      "  '0.001*\"capability\" + 0.001*\"funding\" + 0.001*\"tribunal\" + 0.001*\"cruiser\" + '\n",
      "  '0.001*\"iran\" + 0.001*\"supervision\" + 0.001*\"sector\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model_20th.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of decade summarization algorithm: First, I have created separate data frame for each century. One for 20th and another one for 21st. From the 20th century data frame, I created new str class for each decade and combined all the text of the decade into one row. I repeated the same process for all decades in 20th and 21st century data frame. Then I took all the decade class of a century and created a data frame using those str class. I have done the same thing for both centuries. Once I have the data frame ready, then I implemented the preprocessing of the data and LDA model.\n",
    "\n",
    "### Analysis of the topics: The emerging pattern from these topics is that US wanted to portray itself as a superpower to the rest of the world. Topics highlights few major events in history such as World War I and II, Cold war with Soviet Union, Cuban Missile Crisis, Vietnam War, Drug war in South America, Conflict in Iran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.010*\"american\" + 0.009*\"year\" + 0.008*\"job\" + 0.007*\"new\" + '\n",
      "  '0.007*\"america\" + 0.007*\"people\" + 0.005*\"business\" + 0.005*\"tax\" + '\n",
      "  '0.004*\"world\" + 0.004*\"country\"'),\n",
      " (1,\n",
      "  '0.011*\"america\" + 0.010*\"american\" + 0.008*\"year\" + 0.008*\"people\" + '\n",
      "  '0.007*\"nation\" + 0.006*\"country\" + 0.006*\"new\" + 0.006*\"world\" + '\n",
      "  '0.005*\"tax\" + 0.005*\"congress\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model_21st.print_topics())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
